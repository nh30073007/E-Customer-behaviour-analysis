{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b43d5afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values in Each Column:\n",
      "Restaurant       0\n",
      "Reviewer        38\n",
      "Review           0\n",
      "Rating          38\n",
      "Metadata        38\n",
      "Time            38\n",
      "Pictures         0\n",
      "7514          9999\n",
      "dtype: int64\n",
      "DataFrame after removing rows with missing values:\n",
      "        Restaurant           Reviewer  \\\n",
      "0  Beyond Flavours  Rusha Chakraborty   \n",
      "\n",
      "                                              Review Rating  \\\n",
      "0  ambienc good food quit good saturday lunch cos...      5   \n",
      "\n",
      "                 Metadata             Time  Pictures    7514  \n",
      "0  1 Review , 2 Followers  5/25/2019 15:54         0  2447.0  \n"
     ]
    }
   ],
   "source": [
    "#preprocess task\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "# DATA\n",
    "data_path = r'C:\\Users\\nh013\\Desktop\\Restaurant Reviews\\Restaurant reviews.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# INITIALIZE STOPWORDS, STEMMER, LEMMATIZER\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "# FUNCTION TO PREPROCESS..\n",
    "def preprocess_text(text):\n",
    "    # CHECK IF THE TXT IS A STRING \n",
    "    if isinstance(text, str):\n",
    "        \n",
    "        # ROVOME URLS\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        #REMOVE SPECIAL CHARECTER\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # CONVERT TEXT TO LOWER CASE\n",
    "        text = text.lower()\n",
    "\n",
    "        # TOKENIZE THE TEXT\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS, PERFORM STEMMING AND LEMMATIZATION\n",
    "        filtered_words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return ''  # RETURN AN EAMTY STRING FOR NON STRING VALUES\n",
    "\n",
    "# PERFORM PREPROCESSING FOR REVIEW COL...\n",
    "df['Review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "# FIND MISSING VALUES\n",
    "missing_values = df.isnull().sum()\n",
    "\n",
    "print(\"Missing Values in Each Column:\")\n",
    "print(missing_values)\n",
    "\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# RESET THE INDEX OF THE DATAFRAME  AFTER DROPPING ROWS\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(\"DataFrame after removing rows with missing values:\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c620377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3fd7bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8152cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Review Sentiment\n",
      "0     ambienc good food quit good saturday lunch cos...  positive\n",
      "1     ambienc good pleasant even servic prompt food ...  positive\n",
      "2     must tri great food great ambienc thnx servic ...  positive\n",
      "3     soumen da arun great guy behavior sincereti go...  positive\n",
      "4     food goodw order kodi drumstick basket mutton ...  positive\n",
      "...                                                 ...       ...\n",
      "9995  madhumathi mahajan well start nice courteou se...  positive\n",
      "9996  place never disappoint u food courteou staff s...  positive\n",
      "9997  bad rate mainli chicken bone found veg food ac...  positive\n",
      "9998  person love prefer chines food coupl time husb...  positive\n",
      "9999  check tri delici chines food seen nonveg lunch...  positive\n",
      "\n",
      "[10000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#perform vader.......\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer  \n",
    "\n",
    "# DATA\n",
    "data_path = r'C:\\Users\\nh013\\Desktop\\Restaurant Reviews\\Restaurant reviews.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# INITIALIZE STOPWORDS, STEMMER, LEMMATIZER\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "# FUNCTION TO PREPROCESS..\n",
    "def preprocess_text(text):\n",
    "    # CHECK IF THE TXT IS A STRING \n",
    "    if isinstance(text, str):\n",
    "        \n",
    "        # ROVOME URLS\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        #REMOVE SPECIAL CHARECTER\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # CONVERT TEXT TO LOWER CASE\n",
    "        text = text.lower()\n",
    "\n",
    "        # TOKENIZE THE TEXT\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS, PERFORM STEMMING AND LEMMATIZATION\n",
    "        filtered_words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return ''  # RETURN AN EAMTY STRING FOR NON STRING VALUES\n",
    "\n",
    "# PERFORM PREPROCESSING FOR REVIEW COL...\n",
    "df['Review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "# FUNCTION TO GET SENTIMENT ANALYSIS \n",
    "def get_sentiment_scores(text):\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "#APPLY VADER FOR REVIEW COL....\n",
    "df['Sentiment_Scores'] = df['Review'].apply(get_sentiment_scores)\n",
    "\n",
    "# EXTRACT COMPUND SENTIMENT SCORE\n",
    "df['Compound_Score'] = df['Sentiment_Scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "# DEFINE THRESHOLD FOR CLASSIFY SENTIMENT \n",
    "threshold = 0.2\n",
    "df['Sentiment'] = df['Compound_Score'].apply(lambda x: 'positive' if x > threshold else ('negative' if x < -threshold else 'neutral'))\n",
    "\n",
    "\n",
    "print(df[['Review', 'Sentiment']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f89b5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f073fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nh013\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 18s 156ms/step - loss: 0.7704 - accuracy: 0.7466 - val_loss: 0.7416 - val_accuracy: 0.7494\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 14s 142ms/step - loss: 0.7412 - accuracy: 0.7473 - val_loss: 0.7385 - val_accuracy: 0.7494\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 15s 153ms/step - loss: 0.7363 - accuracy: 0.7473 - val_loss: 0.7317 - val_accuracy: 0.7494\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 16s 161ms/step - loss: 0.7321 - accuracy: 0.7475 - val_loss: 0.7278 - val_accuracy: 0.7494\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 17s 168ms/step - loss: 0.7302 - accuracy: 0.7487 - val_loss: 0.7334 - val_accuracy: 0.7494\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 17s 167ms/step - loss: 0.7189 - accuracy: 0.7492 - val_loss: 0.6430 - val_accuracy: 0.7494\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 18s 179ms/step - loss: 0.5957 - accuracy: 0.7498 - val_loss: 0.5603 - val_accuracy: 0.7281\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 17s 171ms/step - loss: 0.4899 - accuracy: 0.7730 - val_loss: 0.4362 - val_accuracy: 0.8025\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 17s 173ms/step - loss: 0.3996 - accuracy: 0.8256 - val_loss: 0.4235 - val_accuracy: 0.8219\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 17s 170ms/step - loss: 0.3471 - accuracy: 0.8414 - val_loss: 0.4169 - val_accuracy: 0.8231\n",
      "63/63 [==============================] - 3s 32ms/step - loss: 0.4271 - accuracy: 0.8215\n",
      "Test Loss: 0.4271, Test Accuracy: 0.8215\n"
     ]
    }
   ],
   "source": [
    "#perform deep learning model........\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# DATA\n",
    "data_path = r'C:\\Users\\nh013\\Desktop\\Restaurant Reviews\\Restaurant reviews.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "\n",
    "\n",
    "# INITIALIZE STOPWORDS, STEMMER, LEMMATIZER\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "\n",
    "# FUNCTION TO PREPROCESS..\n",
    "def preprocess_text(text):\n",
    "    # CHECK IF THE TXT IS A STRING \n",
    "    if isinstance(text, str):\n",
    "        \n",
    "        # ROVOME URLS\n",
    "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        #REMOVE SPECIAL CHARECTER\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # CONVERT TEXT TO LOWER CASE\n",
    "        text = text.lower()\n",
    "\n",
    "        # TOKENIZE THE TEXT\n",
    "        words = nltk.word_tokenize(text)\n",
    "\n",
    "        # REMOVE STOP WORDS, PERFORM STEMMING AND LEMMATIZATION\n",
    "        filtered_words = [stemmer.stem(lemmatizer.lemmatize(word)) for word in words if word not in stop_words]\n",
    "\n",
    "        return ' '.join(filtered_words)\n",
    "    else:\n",
    "        return ''  # RETURN AN EAMTY STRING FOR NON STRING VALUES\n",
    "\n",
    "# PERFORM PREPROCESSING FOR REVIEW COL...\n",
    "df['Review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "\n",
    "\n",
    "# FUNCTION TO GET SENTIMENT ANALYSIS \n",
    "def get_sentiment_scores(text):\n",
    "    sentiment = analyzer.polarity_scores(text)\n",
    "    return sentiment\n",
    "\n",
    "#APPLY VADER FOR REVIEW COL....\n",
    "df['Sentiment_Scores'] = df['Review'].apply(get_sentiment_scores)\n",
    "\n",
    "# EXTRACT COMPUND SENTIMENT SCORE\n",
    "df['Compound_Score'] = df['Sentiment_Scores'].apply(lambda x: x['compound'])\n",
    "\n",
    "# DEFINE THRESHOLD FOR CLASSIFY SENTIMENT \n",
    "threshold = 0.2\n",
    "df['Sentiment'] = df['Compound_Score'].apply(lambda x: 'positive' if x > threshold else ('negative' if x < -threshold else 'neutral'))\n",
    "\n",
    "# SPLIT DATA INTO TRAINING AND TESTING SET \n",
    "X_train, X_test, y_train, y_test = train_test_split(df['Review'], df['Sentiment'], test_size=0.2, random_state=42)\n",
    "\n",
    "# TOKENIZATION AND SEQUANCE PADDING \n",
    "max_words = 10000  \n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='OOV')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "max_len = 100  # MAXIMUM LENGTH OF SEQUANCE\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_len, padding='post', truncating='post')\n",
    "\n",
    "# ENCODE THE LABEL\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# DEEP LEARNING MODEL \n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_words, output_dim=128, input_length=max_len))\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# COMPILE THE MODEL \n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# TRAIN MODEL \n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "history = model.fit(X_train_padded, y_train_encoded, validation_split=0.2, epochs=10, batch_size=64, callbacks=[early_stopping])\n",
    "\n",
    "# EVALUATE MODLE \n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test_encoded)\n",
    "print(f\"Test Loss: {loss:.4f}, Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67d83a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
